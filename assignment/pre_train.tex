\subsection{基于预训练模型的MRC模型}
% 使用预训练模型可以解决缺少标注数据的问题，因为预训练的模型已经在大量的无标签数据集上学习到了一些重要的特征等，因此应用到
% 具体任务时不需要大量的标注数据，也不需要长时间的训练即可达到很好的效果。
% 语言模型是NLP领域一个非常基础但同时也是非常
% 重要的一项任务，语言模型的目标就是根据一个单词的上下文预测出这个单词。
% 按照预测方式的不同可分为自回归语言模型和自编码语言模型，主要区别在于训练方式上是基于自回归形式还是自编码形式。
预训练模型近年来NLP领域获得了极大的关注度，基于预训练模型的方法在NLP几乎所有的任务上都要优于之前的模型。
预训练方式源自于迁移学习的概念: 首先在其它相关任务上预训练模型，使得模型已经学习到一些知识，然后在目标任务上做进一步优化，
实现模型所学知识的迁移。
对于NLP领域来讲，预训练过程就是在大量的文本数据上学习到通用的语言表示。在应用到下游任务时，预训练所学习到的知识提供了一个很好的
初始化点，从而加快模型的收敛并且提高模型的性能。此外预训练也对模型起到正则化的作用，使得模型避免在数据不充分的数据集上过拟合。

前面介绍的ELMo\upcite{ELMo}就是一个预训练模型，但是预训练的层次比较浅对模型的提升效果也是有限的。
目前流行的几个预训练模型如，
GPT\upcite{GPT}，BERT\upcite{BERT}以及基于BERT改进的预训练模型RoBERTa\upcite{RoBERTa}，UNILM\upcite{UNILM}，ALBERT\upcite{ALBERT}等，从MRC模型结构的角度看这些预训练模型相当于将上述模型通用结构的编码层和交互层融合在一起，在编码的同时进行段落与问题的交互，这些预训练模型刷新了MRC领域多个任务的最佳性能。
%全都是基于语言模型做预训练，因此也叫预训练语言模型。
%预训练的模型根据迁移方式的不同可以分为两种方式基于特征的方式（Feature-based）
%和基于微调的方式（Fine-tuning）。
%基于特征的方式是指对于预训练好的模型，应用到具体任务时固定模型的参数，将具体任务的数据通过预训练的模型得到数据的表示
%特征然后输入到根据具体任务设计的具体模型中。基于微调的方式是广泛采用的一种预训练模型的使用方式，具体的做法是在预训练模型的基础上加上少量的网络层，然后利用具体任务的
%标注数据训练整个网络模型。
%下面将主要概述NLP领域一些流行的预训练模型以及它们在MRC任务上的应用和效果对比。

\subsubsection{Transformer}\label{transformer}
鉴于目前几乎所有的预训练模型都采用transformer\upcite{Transformer}结构或者其变体作为模型的特征提取器，因此本节首先介绍transformer结构。
Transformer是由Vaswani等人提出了一种用于机器翻译的序列到序列（seq2seq）结构。
Encoder端由六个相同的层堆叠而成，每一层有两个子层，第一个子层采用多头（multi-head）自注意力机制，第二个子层采用前馈神经网络（Feed-Forward Network，FFN）构成。
之所以用自注意力机制是因为它既可以捕获句子中每一个单词的全局依赖关系而不受距离影响又可以并行计算。
对比公式（1），自注意力机制下$Q=K=V$，transformer中采用的计算方式如下：
\begin{equation}
\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\end{equation}
其中$\sqrt{d_k}$代表张量维度。此外transformer采用的是多头（multi-head）自注意力机制，将$Q,K,V$三个张量线性映射成多份，每一份之间做注意力的运算最后拼接。
\begin{gather}
	\text{head}_i=\text{Attention}(QW_i^Q,KW_i^K,VW_i^V) \notag \\
	\text{Multi-head}(Q,K,V)=\text{concat}(\text{head}_1,\text{head}_2,\cdots,\text{head}_h)W^o
\end{gather}
其中$h$代表头的数目，是一个超参数，$W_i^Q,W_i^K,W_i^V,W^o$都是训练参数。

采用multi-head的目的是让模型联合关注序列中不同位置单词的不同表示子空间的信息，可以类比于卷积神经网络中利用多个卷积核做特征提取，目的同样是使得不同的卷积核关注的不同的特征。
此外
每一个子层都利用层正则化（layer normalization\upcite{layerNormal}）和
残差连接（residual connection\upcite{RL}）机制。
Decoder端与Encoder端类似，区别在于每一层额外添加了encoder-decoder注意力。

%Transformer的整体架构
%通过利用自注意力（self-attention）机制取代RNN那种序列式的计算方式，
%对于一个句子中的两个单词不考虑单词之间顺序的关系，直接计算它们之间的相关度，例如计算两个单词向量表示的内积。
%自注意力机制可以捕获句子中长距离依赖的特征关系，
%解决了循环神经网络固有的序列式传递信息导致后面的单词与前面的单词
%之间达不到有效的信息传递问题。
%通过自注意力机制不仅可以做到
%单词之间的全局交互同时其并行计算使得模型训练时间大幅减少。
Transformer的encoder端和decoder端都可以做特征提取器如BERT\upcite{BERT}用encoder端特征提取，GPT\upcite{GPT}用decoder端特征提取，实验证明在大规模数据集上transformer的特征提取能力要强于
基于RNN变体的编码器，目前几乎所有的NLP预训练模型都是利用transformer作为特征提取器。


\subsubsection{预训练模型}\label{pretrain}
%\subsubsection{相关预训练模型}
%ELMo\upcite{ELMo}是在2018年提出的一种预训练语言模型。
%传统的词嵌入模型如Word2Vec\upcite{word2vec},GloVe\upcite{GloVe}属于
%静态的词向量，训练好模型后一个单词的表示
%向量就是固定的，没有考虑上下文的信息，因此无法解决多义词问题。
%ELMo提出一个三层网络的模型
%，第一层就是词嵌入层用来提取单词特征，随后是两层BiLSTM网络分别提取
%单词的词性特征和语义特征。前向LSTM的目标是根据前$k-1$个词预测第$k$个词，从而计算出
%一个句子的概率，如公式（10）。反向LSTM的目标是根据最后的单词直到第$k+1$个单词预测第$k$个单词，
%具体如公式（11）。
%\begin{gather}
%    p(t_1,t_2,\cdots,t_N)=\prod_{k=1}^{N}p(t_k|t_1,t_2,\cdots,t_{k-1})\\
%    p(t_1,t_2,\cdots,t_N)=\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},\cdots,t_{N})
%\end{gather}
%最后的目标函数是最大化联合的前向和后向最大似然：
%\begin{equation}
%    \begin{split}
%    L(\Theta)&=\sum_{k=1}^{N}(\log p(t_1,\cdots,t_{k-1};\Theta)) \\
%        &+\log p(t_{k+1},\cdots,t_N;\Theta)
%    \end{split}
%\end{equation}
%在做阅读理解任务时，将文章和问题输入到模型中，每一层都会得到句子的语义表示，
%然后将每一层的特征加权求和作为ELMo的输出，此时得到的每一个单词的
%向量表示都是考虑了上下文的，将其作为下游模型嵌入层的输入。
%利用ELMo+BiDAF\upcite{BiDAF}
%结构超过之前单模型8.3个百分点。可以看出ELMo属于自回归语言模型，模型的迁移方式是基于特征的方式。




%\subsubsection{GPT}
OpenAI\upcite{GPT}提出一种生成式预训练模型（Generative Pre-Trained，GPT），使用多层的transformer\upcite{Transformer}的解码端作为特征提取器。模型采用两阶段的训练方式，先利用大规模无监督语料库训练前向语言模型，
然后在下游任务的少量监督数据集上微调模型，因此
GPT也是一种半监督学习方式，
%采用单向语言模型作为训练的任务，
预训练阶段的目标函数就是标准的单向语言模型的目标函数，见公式（1）。
%\begin{equation}
%    L(\Theta)=\sum_{k=1}^{N}(\log p(t_1,\cdots,t_{k-1};\Theta))
%\end{equation}
%GPT与ELMo的共同点是它们都属于自回归语言模型，
%而最大的不同之处在于迁移的设计上并不是像ELMo那种基于特征方式的迁移。GPT有统一的输入数据表示形式，
%而且在输入层并不需要继续设计复杂的网络模型而仅仅只需要简单的结构，在训练时整个网络模型的参数共同训练。
%这种基于微调的迁移方式被后续的其它预训练模型广泛采用。
%利用GPT在RACE\upcite{RACE}数据集上微调后达到的效果比之前最好的模型提高了5.7个百分点。
GPT是NLP领域首先提出的一种基于微调（fine-tune）的通用式网络结构，不仅仅在MRC领域，在很多其它的NLP领域都取得了很大的
进步。
%\end{multicols}
% \begin{center}
%     \textbf{表２　预训练语言模型对比}\\
%     \vspace{10pt}
%     \begin{tabular}{c c c c}
%         \toprule
%         模型&任务&模型结构&介绍 \\
%         \midrule
%         ELMo&单向LM&LSTM&拼接两个单向语言模型的语义信息，
                
%         基于特征形式迁移 \\
%         \midrule
%         GPT&单向LM&Transformer&首次采用预训练＋微调形式
        
%         的两阶段任务，特征提取器采用transformer \\
%         \midrule
%         BERT&双向LM＋NSP&Transformer&利用掩码语言模型和预测下一个句子共同作为训练任务 \\
%         \midrule
%         UNILM&单向LM+双向LM+Seq2SeqLM&Transformer&同时训练三种语言模型，采用掩码机制解决不同语言模型的约束问题 \\
%         \midrule
%         ALBERT&双向LM+SOP&Transformer&对比BERT采用矩阵分解和共享参数减少模型的参数量同时用句子顺序预测取代下一个句子预测 \\
%         \bottomrule
%     \end{tabular}
% \end{center}


%\subsubsection{BERT}
GPT属于自回归语言模型，自回归语言模型的缺点就是由于自回归的性质使得它不能同时利用一个单词的
上下文信息预测这个单词
Devlin等人\cite{BERT}提出BERT预训练模型，
与GPT最本质的不同在于预训练方式上采用的降噪自编码方式，
随机掩盖
掉一些单词，在输出层获得掩盖位置的概率分布，让模型根据掩盖位置的上下文预测这个单词，
这种机制也叫掩码语言模型（Masked Language Model，MLM)或双向语言模型。
%自回归语言模型的缺点就是由于自回归的性质使得它不能同时利用一个单词的
%上下文信息预测这个单词，
%ELMo虽然利用双向LSTM来预测单词但是这也只是两个单向的语言
%模型的拼接并不能当做双向语言模型。
%BERT在整个预训练的流程上采用GPT的方式，即预训练然后微调。
%但是BERT采用降噪自编码（DAE）的方式训练，具体的就是在输入数据中加入噪声，也就是随机掩盖
%掉一些单词，让模型根据掩盖掉单词的上下文预测这个单词，这种训练方式也叫掩码语言模型（MLM)。
%对比ELMo和GPT的目标函数，BERT的掩码语言模型的
MLM的目标函数为：
\begin{equation}
    \begin{split}
        L(\Theta)=\sum_{i=1}^{N}\log P(t_k|t_1,t_2,\cdots,t_{k-1},t_{k+1},\cdots,t_{N})
    \end{split}
\end{equation}
除了MLM任务外，还利用下一个句子预测（Next Sentence Prediction，NSP）任务使得模型
在诸如文本蕴含、问答这类需要判断两个句子关系的下游任务表现更好。
BERT的预训练过程实质上是
一个多任务学习的过程，通过MLM和NSP两个任务
提高了预训练模型的语义表达能力，其中MLM任务用来学习句子中词与词之间的语义关联而NSP任务用来学习两个句子之间的逻辑关系。
BERT在SQuAD\upcite{SQuAD1}数据集上的效果超过了人类的水平，在其它的NLP任务上也都有提升。
%\subsubsection{UNILM}

BERT开启了NLP领域预训练模型的时代，此后很多更加强大的预训练模型相继提出。这些预训练模型从结构上主要分为两大类：基于BERT的改进模型和XLNet。基于BERT的改进模型主要是针对BERT预训练阶段的两个任务MLM和NSP做改进，如
Liu等人\upcite{RoBERTa}提出RoBERTa模型，使用动态掩码替换BERT的静态掩码同时去除NSP任务；Li等人\upcite{UNILM}提出UNILM模型扩展了BERT预训练的任务，由于双向语言模型的性质使得BERT在生成任务上效果不好，
UNILM同时训练单向语
言模型（包括从左到右和从右到左）、双向语言模型以及Seq2Seq语言模型，使用掩码机制来解决不同的语言模型
约束问题；Lan等人\upcite{ALBERT}提出ALBERT模型，利用句子顺序预测（Sentence Order Prediction，SOP）任务改进了BERT的NSP任务；除了在预训练阶段改进训练任务外，Liu等人\upcite{MTDNN}提出MT-DNN模型在微调阶段引入了多任务学习机制，使用多个任务来微调模型参数使得模型具有更好的泛化型。

BERT模型以及基于BERT结构改进的模型在预训练阶段都采用降噪自编码的思想，所带来的问题就是预训练过程与微调过程不匹配，预训练使用的掩码机制在下游任务微调时不会被使用，导致两个过程存在数据分布的偏差。自回归语言模型虽然不存在这个问题，但是缺点是不能同时利用上下文信息。Yang等人\upcite{XLNet}提出的XLNet模型是一种可以同时获得上下文信息的自回归语言模型，
通过使用排列语言模型，引入双向自注意力流和循环机制兼顾了自回归和自编码语言模型的优点。

%
%BERT这种降噪自编码的方式虽然可以达到双向的利用上下文信息，但是由于其在预训练过程中对输入数据加入掩码而在微调时又不会加入掩码，导致
%了预训练过程与微调过程不匹配，存在一定的数据分布偏差，此外BERT对于屏蔽词的预测是独立的。基于上述问题，文献\cite{XLNet}提出一种新的
%预训练模型XLNet，它是一种可以获得双向的上下文信息的自回归语言模型，克服了传统的自回归语言模型和自编码语言模型各自的问题。XLNet采用的是
%排列语言模型（Permutation Language Model，PLM）
%，自回归模型中利用文本的前向或者后向序列的最大似然来建模，而PLM排列这个文本所有可能
%的序列顺序，仍然利用自回归语言模型的目标函数，但是综合所有可能的排列后每一个单词都可以获得双向的上下文信息。此外XLNet
%借鉴transformer-XL\upcite{Transformer-XL}中的片段循环机制引入循环机制可以捕获更长的句子依赖关系。

%文献\cite{RoBERTa}提出一种基于BERT改进其训练方式的预训练模型RoBERTa，其改进的方式包括：使用动态掩码替换静态掩码、去除NSP任务、使用更大的batch、
%更多的训练语料以及更长的训练时间。
%其中动态掩码是指对于输入数据中随机掩盖的单词并不是固定的，因为BERT的掩码机制是静态的，即对于每一个输入序列一旦选定了其中的
%的某个单词将其屏蔽，那么之后的整个训练过程该单词始终被掩盖。RoBERTa提出将输入数据复制10份，每一份都是随机掩盖部分单词，这样同一个输入序列
%就会有10种不同的掩码方式，从而达到动态掩码的目的。





%
%
%UNILM\cite{UNILM}扩展了BERT预训练的任务，由于双向语言模型的性质使得BERT在生成任务上效果不好，
%UNILM同时训练单向语
%言模型（包括从左到右和从右到左）、双向语言模型以及Seq2Seq语言模型，使用掩码机制来解决不同的语言模型
%约束问题。虽然是三个不同的语言模型作为训练任务但是共享同一个网络结构，也就是利用三个任务来联合的优化
%模型的参数。这种多任务学习的方式缓解了模型在某一个单一任务上容易出现过拟合的问题，
%使得预训练后的模型不仅在原有的自然语言理解任务上效果进一步提升，
%同时在自然语言生成任务上也达到了很好的效果。
%在微调阶段，对于自然语言理解任务（如文本分类、抽取式问答等）同BERT的微调方式一样，对于自然语言
%生成任务（如自动化摘要、生成式问答等）采用与预训练阶段Seq2Seq语言模型类似的方式在目标序列中
%随机的掩盖掉一些单词从而让模型根据源序列生成目标序列的单词达到生成任务的目的。
%UNILM不仅在抽取式QA数据集（如SQuAD）上超过BERT，在生成式QA数据集（如CoQA）上的表现远超过最初的基准模型。




%ALBERT\upcite{ALBERT}改进了BERT的NSP任务，BERT的NSP任务包含了两个子任务，来源于同一篇文章的两个连续的句子判别为正例，来源于不同文章的句子判别为负例，
%这使得模型不能集中于判断两个句子之间的顺序反而更加关注句子所表达的主题是否一致。因此ALBERT中用
%SOP(sentence-order prediction)取代NSP任务，SOP是指句子顺序预测，两个句子都是
%来源于同一篇文章的连续的句子，调换顺序后便是负例，这使得模型集中于预测句子之间的顺序关系，
%实验表明SOP任务使得模型的效果提升一个百分点。
%ALBERT的改进机制使得模型的预训练后的效果更好，在RACE\upcite{RACE}，SQuAD 2.0\upcite{SQuAD2}等
%机器阅读理解数据集上的准确率超过BERT以及其它的预训练模型。
表11从预训练任务、模型采用的特征提取器等详细对比了本文介绍的所有预训练模型。表12对比了几个预训练模型在两个常用的MRC数据集上的表现。
% \subsection{小结}
% 传统的静态的预训练词向量虽然可以给模型性能带来一定程度的提升，但是这种提升非常有限。主要原因是
% 其静态的性质无法解决多义词的
% 问题而且所学习到的语义信息也是浅层的，仍然需要模型在标注数据集上从头学习。
% ELMo的出现使得静态词向量的问题得以解决，并且这种利用语言模型作为训练任务的方式也被后续的其它
% 预训练模型采用。GPT首次提出预训练过程结合微调过程的这种两阶段方式，使得不需要设计复杂的网络模型即可在
% 多个NLP任务上达到显著地提升。BERT是NLP预训练史上重要的里程碑模型，后续的其它更加优秀的
% 预训练模型如ALBERT、UNILM也都是在BERT的基础上进行进一步优化如：引入生成任务、改进训练方式等。

\begin{table}[ht]
	\caption{预训练模型对比\\ Table 11 Comparison of pre-trained model}
	\centering
	%\vspace{10pt}
	%表格超出页边距用resizebox
	\resizebox{\textwidth}{!}{
		\begin{tabular}{c c c c}
			\toprule
			模型&任务&模型结构&介绍 \\
			\midrule
			ELMo\upcite{ELMo}&\tabincell{c}{前向LM \\ 反向LM}&LSTM&\tabincell{c}{拼接两个单向语言模型的语义信息，\\ 基于特征形式迁移} \\
			\midrule
			GPT\upcite{GPT}&前向LM&Transformer&首次采用预训练＋微调形式 \\
			\midrule
			BERT\upcite{BERT}&MLM+NSP&Transformer&利用掩码语言模型（MLM）和下一句预测（NSP）共同作为训练任务 \\
			\midrule
			MT-DNN\upcite{MTDNN}&MLM+NSP&Transformer&预训练过程与BERT一致，微调阶段采用多任务学习 \\
			\midrule
			XLNet\upcite{XLNet}&PLM&Transformer-XL&\tabincell{c}{采用排列语言模型（PLM），双向自注意力流\\模型以自回归方式训练但是基于上下文预测} \\
			\midrule
			RoBERTa\upcite{RoBERTa}&MLM&Transformer&采用动态掩码机制，去除NSP任务 \\
			\midrule
			UNILM\upcite{UNILM}&\tabincell{c}{前向LM+反向LM\\ +MLM\\ +Seq2SeqLM}&Transformer&\tabincell{c}{同时训练多种语言模型，\\ 采用掩码机制解决不同语言模型的约束问题} \\
			\midrule
			ALBERT\upcite{ALBERT}&MLM+SOP&Transformer&\tabincell{c}{对比BERT采用矩阵分解和共享参数减少模型的参数量，\\ 同时用句子顺序预测任务（SOP）取代下一个句子预测任务（NSP）} \\
			\bottomrule
		\end{tabular}
	}
\end{table}

 \begin{table}[ht]
     \centering
     \caption{预训练模型对比}
     \begin{tabular}{l c c}
         \toprule
         \multirow{2}{*}{模型}& SQuAD 2.0\upcite{SQuAD2} & RACE\upcite{RACE}\\
         \cmidrule(lr){2-2} \cmidrule(lr){3-3} 
         &EM/F1& Acc \\
         \midrule
         $\text{GPT}_{v1}$\upcite{GPT}&-&59.0\\
         \midrule
         $\text{BERT}_{large}$\upcite{BERT}& 80.0/83.1&72.0\\
         \midrule
         XLNet\upcite{XLNet}&86.4/89.1 &81.8\\
         \midrule
         RoBERTa\upcite{RoBERTa}&86.8/89.8 &83.2\\
         \midrule
         ALBERT\upcite{ALBERT}&88.1/90.9 &86.5\\
         \bottomrule
     \end{tabular}
 \end{table}


%\subsubsection{的MRC模型}
%在预训练模型出现后不仅仅在MRC任务上，在其它的NLP任务上模型的结构都发生了较大的变化，
%在模型的基础上利用具体任务的数据微调模型即可达到很好的效果。
%%但是预训练模型毕竟是一个通用式的模型，
%%它给了模型很好的初始化参数，不过要想达到更好的效果仍然要根据具体任务的形式来设计模型。
%下面列举几个MRC数据集上目前效果较好的基于预训练模型的MRC模型。
%
%人在做阅读理解问题的时候通常会先带着问题大致的浏览一下这篇文章，对这篇文章的含义有一个大致的了解。之后再根据问题详细的阅读文章寻找答案。受到这种阅读形式的启发，Zhang等人\upcite{Retrospective}提出一种回顾式阅读器（Retrospective Reader，Retro-Reader）模型。整个模型由两个步骤构成：（1）第一步先简要的略读文章，建模文章与问题的大致关联给出初步的判断该问题是否可以回答。（2）第二步是精读模块，目的是验证可回答性并且给出最终判断。模型的编码器采用强大的预训练模型ALBERT。Retro-Reader在SQuAD 2.0\upcite{SQuAD2}数据集上显著优于其它模型。
%
%%如何训练出一个性能更加强大的预训练模型成为近年来NLP领域的研究热点。
%Zhang等人\upcite{DCMN}
%利用BERT\upcite{BERT}和XLNet\upcite{XLNet}作为编码器同时采用文献\cite{Co-matching}提出的co-matching方法提出了DCMN模型，在RACE\upcite{RACE}
%数据集上达到了很高的准确率。
%而后在DCMN的基础上引入选项交互模块和段落选择模块提出DCMN+\upcite{DCMN+}模型，模型的性能进一步提升。
%
%BERT以及基于BERT改进的预训练模型其数据输入形式只能是两个句子的拼接因此并不适合直接处理CoQA这种对话型阅读理解数据集。
%%Qu等人\upcite{HAE}提出一种简单而有效的模型，仅仅需要在BERT模型的输入端为每一个单词添加两个额外的向量用来表明这个单词有没有在历史答案中出现过，文中称这两个向量为（History Answer Embedding，HAE）。实验表明BERT+HAE模型较之前的模型可以处理更多的历史对话信息。
%Zhu等人\upcite{SDNet}
%提出SDNet模型，以基于特征的方式迁移BERT作为编码器，同时将之前轮次的问题和答案拼接到当前轮的问题上构成一个新问题。模型采用自注意力机制获得历史对话信息之间的交互语义，具体的计算方式采用FusionNet\upcite{Fusionnet}模型提出的融合方法。
%Ohsugi等人\upcite{simpleqa}以基于微调的方式迁移BERT模型。
%将历史对话信息每一轮的问题与答案分别与文章连接送入
%BERT，将每一个BERT的输出连接作为输出层的输入。两个模型在CoQA\upcite{CoQA}和QuAC\upcite{QuAC}两个对话型数据集上的效果均超过前面的BiDAF++\upcite{Clark}，FlowQA\upcite{FlowQA}等利用复杂交互机制的模型。
%由于预训练模型UNILM\upcite{UNILM}改进了BERT的训练任务，
%增加了自回归语言模型以及seq2seq语言模型使得其
%在生成式任务上的效果很好，在CoQA数据集上远远的超过于Reddy等\upcite{CoQA}提出的基准模型。




\subsubsection{迁移预训练模型}
预训练模型具有强大的文本表征能力，但是在应用到机器阅读理解任务上还需要根据具体的数据集特点设计不同的微调网络结构。以BERT系列的预训练模型为例，预训练阶段和微调阶段除了输出层外共享相同的体系结构，在做抽取式阅读理解任务时，输出层最简单的设计方式是仅利用两个向量与段落的向量表示计算答案在段落中起始位置和终止位置的概率。
对于其它形式的阅读理解任务，往往根据任务的特点在输入层和输出层设计相应的结构，相关的工作可以参照\upcite{HAE,simpleqa,Retrospective}。
另一种迁移预训练模型的方式是基于特征形式的迁移，将具体任务的数据送进预训练模型，预训练模型的特征表示作为下游模型的输入，训练时固定预训练模型的参数。


%预训练模型根据迁移方式的不同可以分为两种方式：（1）基于特征的方式（Feature-based）
%（2）基于微调的方式（Fine-tuning）。
%基于特征的方式是指将具体任务的数据送进预训练模型，将得到的文本表示
%特征输入下游模型结构中。
%
%基于微调的方式是广泛采用的一种预训练模型的使用方式，具体的做法是在预训练模型的基础上加上少量的网络层，然后利用具体任务的
%标注数据训练整个网络模型。



虽然设计一个强大的预训练模型具有更好的泛化型和应用价值，但是预训练模型所消耗的计算资源是巨大的，
因此如何利用预训练模型结合具体任务改进模型的结构是至关重要的。
% \begin{center}
%     \begin{tabular}{l c}
%         \toprule
%         单模型&EM/F1 \\
%         \midrule
%         MatchLSTM with Pointer Network& 64.7/73.7 \\
%         \midrule
%         Dynamic Coattention Network& 66.2/75.9 \\
%         \midrule
%         BiDAF&68.0/77.3 \\
%         \midrule
%         R-Net&72.3/80.7 \\
%         \midrule
%         QANet(data augmentation x3)& 76.2/84.6\\
%         \midrule
%         ELMo+BiDAF&78.6/85.8\\
%         \midrule
%         $\text{BERT}_{large}$(+TriviaQA)& 85.1/91.8 \\
%         \bottomrule
%     \end{tabular}
% \end{center}
% \begin{table*}[!hp]
%     \centering
%     \begin{tabular}{l c c c c}
%         \toprule
%         &SQuAD 1.1(F1)&SQuAD 2.0(F1)&RACE(Acc)&CoQA \\
%         \midrule
%         ELMo+BiDAF&85.8&-&-&- \\
%         GPT1&
%         \bottomrule
%     \end{tabular}
% \end{table*}

% \begin{table}
%     \begin{tabular}{l c}
%         \toprule
%         训练任务&目标函数 \\
%         \midrule
%         前向语言模型&$P(t_1,t_2,\cdots,t_N)=\prod_{k=1}^{N} P(t_k|t_1,t_2,\cdots,t_{k-1})$ \\
%         \midrule
%         反向语言模型&$P(t_1,t_2,\cdots,t_N)=\prod_{k=1}^{N} P(t_k|t_{k+1},t_{k+2},\cdots,t_N)$ \\
%         \midrule
%         掩码语言模型&$P(t_1,t_2,\cdots,t_N)=\prod_{k=1}^{N} P(t_k|t_1,t_2,\cdots,t_{k-1},t_{k+1},t_{k+2},\cdots,t_N)$ \\
%         \bottomrule
%     \end{tabular}
% \end{table}

% \subsubsection{迁移预训练模型}
% 训练好一个模型后应当考虑如何迁移这个预训练模型到具体的任务上。
% 按照迁移形式的不同可以分为（1）基于特征的迁移方式和（2）基于微调的迁移方式。
% % 基于特征的迁移方式是指将数据输入到预训练模型中然后提取出模型对输入数据的特征表示
% % ，将这组特征作为下游任务模型的输入，训练时预训练模型的参数是固定的。这种基于特征的迁移形式
% % 相当于把预训练模型当做语义编码器，上层仍需要设计具体的模型。

% % 基于微调的迁移方式是指将数据输入到预训练模型中，然后将预训练模型的输出作为
% ELMo\upcite{ELMo}是典型的基于特征的迁移方式，

% GPT\upcite{GPT}是典型的
