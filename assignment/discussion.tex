\section{讨论}
%本章主要讨论MRC领域的发展历史和目前MRC领域存在的主要问题
%本章介绍了MRC任务的定义以及根据答案类型的不同划分四种任务并且介绍了每一个任务的形式以及相关的数据集
%，同时简述了每一个任务下衡量模型性能所用的评估指标。
%下表5从数据集来源及规模，文章、问题以及答案的类型角度上对比了本章所介绍的所有数据集
\subsection{回顾MRC的发展历史}
从MRC数据集的发展角度看，从最简单的填空型数据集，到抽取型数据集再到复杂的需要从多段落中归纳答案的数据集等
每一个新的数据集都会在原有数据集的基础上增加各种各样的难度，从而不得不设计更加优秀的模型处理这些任务
。
从模型内部各个层次的发展角度上看，
%词嵌入层从早期最常用的预训练静态词向量Word2Vec\upcite{word2vec}及GloVe\upcite{GloVe}发展到基于上下文的动态词嵌入技术ELMo\upcite{ELMo}，编码层从最常用的循环神经网络及其变体（如LSTM\upcite{LSTM}等）发展到更强大的特征提取器transformer\upcite{Transformer}。
嵌入层和编码层在预训练模型出现之前并没有较大的变动。各个模型主要集中在交互层的注意力机制上，为了获得更全面的交互信息从单向发展到双向，为了可以达到多步推理的目的从单跳结构发展到多跳结构。
而对于更加复杂的阅读理解任务时，模型整体发生了较大的改动。面对无答案的阅读理解任务设计答案验证模块来验证问题是否可以回答，面对多段落型阅读理解任务设计段落选择模块减小答案搜索范围，面对对话形式的阅读理解任务通过将之前轮的对话信息以文本拼接或者信息流动的方式使得模型在回答当前轮问题的时候可以关注到之前的对话信息。
%Match-LSTM\upcite{MatchLSTM}等改进到双向（如BiDAF\upcite{BiDAF}、DCN\upcite{DCN}等），单层（如）

%在预训练模型出现后不仅仅在MRC任务上在其它的NLP任务上模型的结构都发生了较大的变化，
%%在模型的基础上利用具体任务的数据微调模型即可达到很好的效果，
%如何训练出一个性能更加强大的预训练模型成为近年来NLP领域的研究热点。
%但是预训练模型只是给了更好的初始化参数，
%如果想要进一步提升模型的性能还需要在其基础上根据具体的任务设计一个更好的模型。
%如Zhang等人\upcite{DCMN}
%利用BERT\upcite{BERT}和XLNet\upcite{XLNet}作为编码器同时采用文献\cite{Co-matching}提出的co-matching方法提出了DCMN模型，在RACE\upcite{RACE}
%数据集上达到了很高的准确率。
%而后在DCMN的基础上引入选项交互和段落选择任务使得
%如何利用预训练模型结合具体任务改进模型的结构是至关重要的。
%表8对比了本章所介绍的模型在三个常用数据集上的表现，可以看到基于预训练
%模型的效果要显著的优于其它模型，此外在其它数据集上排名靠前的模型几乎都是基于预训练模型的。

%带有不可回答问题的任务，多段落型任务以及对话型问答任务，这三个任务相比于之前阅读理解任务都有各自的特点和难点所在。实验表明之前的经典模型在比较简单的数据集上已经达到了很好的效果，然而迁移到这几个复杂的阅读理解任务上模型效果显著下降。如何设计性能更加强大的模型来处理更加复杂的任务还需要大量的研究。



%正是这些越来越具有挑战性的数据集的发布，极大的推动了MRC领域的发展。


\subsection{分析MRC面临的主要问题}
预训练模型出现后基于神经网络的MRC模型的性能再一次都取得了显著的进步，但是目前MRC领域还有一些问题需要解决。

\textbf{模型缺乏推理能力} \quad
如前面所述，基于注意力机制的匹配模型大多是浅层的语义匹配模型，基于多跳结构的推理模式还过于单一，这些均没有形成深层次的阅读理解模型。Jia等人\upcite{Jia}在SQuAD数据集的基础上设计对抗样本来测试MRC模型，在文章中加入人工设计的句子，这些句子与问题相似但是与答案无关以此用来误导模型。这些误导的句子对人来说没有什么影响然而实验证明几乎所有模型的准确率都显著下降，这表明模型靠的是关键词匹配方式寻找答案而真正的阅读理解能力和推理能力很弱，让模型具有较强的推理能力至关重要。

\textbf{答案生成技术不足} \quad
生成答案的技术还需要进一步提升，回顾目前机器阅读理解领域的数据集以及相应的模型，
大多集中于片段抽取式问答且模型准确度很高，
而对于自由答案型这种需要生成答案的模型效果很差，有些模型直接将生成式问题转为抽取式问题效果反而优于生成式的做法，主要原因
在于生成答案模块的效果不够好。

\textbf{模型缺乏外部知识} \quad
目前很少有机器阅读理解模型融合外部知识，都是直接根据给定的文档回答相关的问题，而人在阅读一篇文章的时候对这篇文章的理解程度和他已经掌握的知识水平有很大关系。因此如果将外部知识源融入模型中，模型的性能大概率会显著提高。

\subsection{探索MRC未来的研究方向}

\textbf{如何设计高质量的数据集} \quad
前面介绍了很多MRC领域常用的数据集，也同时分析了各个数据集的难度、特点、规模等。虽然目前的数据集已经不像之前的数据集模型仅仅通过浅层的匹配机制就能够达到很高的效果，但是一个高质量的机器阅读理解数据集不应该是普通的大规模、高难度的特点，它要能够准确的考察模型是否具有人类阅读能力，包括推理能力，常识能力等。

机器阅读理解赋予了计算机阅读理解文本的能力，在搜索、对话、医疗以及教育领域都有着广阔的应用空间，未来的研究方向有以下几点值得关注：

1）目前机器阅读理解领域的数据集大多是通用领域方向，而设计专业领域数据集也尤为重要。比如通过利用机器阅读理解技术分析产品说明文档和用户的问题语义从而解决用户对产品的问题达到智能客服服务，或者在医疗诊断中分析大量病例和知识库提供智能医疗服务。


2）目前机器阅读理解主要集中于非结构化的文本领域，而
还有许多其它结构、不同模态的数据如表格、视频、音频、图片等，相关的研究方向如数据库问答，视觉问答等。
多模态阅读理解模型是未来的机器阅读理解发展方向之一。

%3）目前很少有机器阅读理解模型融合外部知识，都是直接根据给定的文档回答相关的问题，而人在阅读一篇文章的时候对这篇文章的理解程度和他已经掌握的知识水平有很大关系。因此如果将外部知识源融入模型中那么模型的性能大概率会显著提高。





















%
%\begin{thebibliography}{99}
%    \bibitem{lstm}Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
%    1735–1780, 1997.
%
%    % \bibitem{BLEU}Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for au-
%    % tomatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on
%    % Association for Computational Linguistics, pages 311–318. Association for Computational
%    % Linguistics, 2002.
%    \bibitem{ROUGE}Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summariza-
%    tion Branches Out, 2004.
%
%    \bibitem{Maxout Network}Goodfellow, I.J., Warde-Farley, D., Mirza, M., Courville, A., Bengio, Y.: Maxout networks.
%    arXiv preprint arXiv:1302.4389 (2013)
%
%    \bibitem{word2vec}Mikolov T，Sutskever I，Chen K，et al.Distributed represen-
%    tations of words and phrases and their compositionality[C]//
%    Advances in Neural Information Processing Systems，
%    2013：3111-3119.
%
%    \bibitem{GRU}Junyoung Chung, Caglar Gulcehre,
%    Kyunghyun Cho, and Yoshua Bengio. 2014. Empir-
%    ical Evaluation of Gated Recurrent Neural Networks
%    on Sequence Modeling. arXiv, pages 1–9.
%
%    \bibitem{GloVe}Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for
%    word//w representation. In Empirical Methods in Natural Language Processing (EMNLP), pp.
%    1532–1543, 2014.
%
%    \bibitem{neural machine translation by jointly learning to align and translate}Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
%    learning to align and translate. ICLR, 2015.
%
%    \bibitem{memory network}J. Weston, S. Chopra, and A. Bordes. Memory networks. In International Conference on
%    Learning Representations (ICLR), 2015.
%
%    \bibitem{Highway Network}Srivastava, R.K., Greff, K., Schmidhuber, J.: Highway networks.
%    arXiv preprint
%    arXiv:1505.00387 (2015)
%
%    \bibitem{MemN2N}Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In
%    Advances in neural information processing systems, pages 2440–2448, 2015.
%
%    \bibitem{Teaching Machines to Read and Comprehend}Hermann, K.M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., Blun-
%    som, P.: Teaching machines to read and comprehend. In: Advances in Neural Information
%    Processing Systems, pp. 1693–1701 (2015)
%
%    \bibitem{AR}Danqi Chen, Jason Bolton, and Christopher D Manning. A thorough examination of the
%    cnn/daily mail reading comprehension task. In Proceedings of the 54th Annual Meeting of
%    the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages
%    2358–2367, 2016.
%
%    \bibitem{ASR}Rudolf Kadlec, Martin Schmid, Ondřej Bajgar, and Jan Kleindienst. Text understanding
%    with the attention sum reader network. In Proceedings of the 54th Annual Meeting of the
%    Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 908–
%    918, 2016.
%
%    \bibitem{IAReader}Alessandro Sordoni, Philip Bachman, Adam Trischler, and Yoshua Bengio. Iterative alternat-
%    ing neural attention for machine reading. arXiv preprint arXiv:1606.02245, 2016.
%
%    \bibitem{SQuAD1}Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
%    for machine comprehension of text. In Proceedings of the Conference on Empirical Methods in
%    Natural Language Processing, 2016.
%
%    \bibitem{sentinel vector}Merity, S., Xiong, C., Bradbury, J., Socher, R.: Pointer sentinel mixture models. arXiv
%    preprint arXiv:1609.07843 (2016)
%
%    \bibitem{Deepwise Separable convolution}François Chollet. Xception: Deep learning with depthwise separable convolutions.
%    abs/1610.02357, 2016.
%
%    \bibitem{Machine comprehension using match-lstm and answer pointer}Wang, S., Jiang, J.: Machine comprehension using match-lstm and answer pointer. arXiv
%    preprint arXiv:1608.07905 (2016)
%
%    \bibitem{Dynamic coattention networks for question answering}Xiong, C., Zhong, V., Socher, R.: Dynamic coattention networks for question answering.
%    arXiv preprint arXiv:1611.01604 (2016)
%
%    \bibitem{Bidirectional attention flow for machine comprehension}Seo, M., Kembhavi, A., Farhadi, A., Hajishirzi, H.: Bidirectional attention flow for machine
%    comprehension. arXiv preprint arXiv:1611.01603 (2016)
%    
%    \bibitem{RNet}Wang, W., Yang, N., Wei, F., Chang, B., Zhou, M.: Gated self-matching networks for
%    reading comprehension and question answering. In: Proceedings of the 55th Annual Meet-
%    ing of the Association for Computational Linguistics (Volume 1: Long Papers), vol. 1, pp.
%    189–198 (2017)
%
%    \bibitem{Reasonet}Shen, Y., Huang, P.S., Gao, J., Chen, W.: Reasonet: Learning to stop reading in machine
%    comprehension. In: Proceedings of the 23rd ACM SIGKDD International Conference on
%    Knowledge Discovery and Data Mining, pp. 1047–1055. ACM (2017)
%
%    \bibitem{GAReader}Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William Cohen, and Ruslan Salakhutdinov.
%    Gated-attention readers for text comprehension. In Proceedings of the 55th Annual Meeting
%    of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1832–1846,
%    2017.
%
%    \bibitem{PGNet}Abigail See, Peter J. Liu, and Christopher D.
%    Manning. 2017. Get to the point: Summa-
%    rization with pointer-generator networks. In
%    Annual Meeting of the Association for Compu-
%    tational Linguistics (ACL), pages 1073–1083.
%    Vancouver, Canada.
%
%    \bibitem{TriviaQA}Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: A large scale
%    distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th
%    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
%    volume 1, pages 1601–1611, 2017.
%
%    \bibitem{DrQA}Danqi Chen, Adam Fisch, Jason Weston, and
%    Antoine Bordes. 2017. Reading Wikipedia
%    to answer open-domain questions. In Asso-
%    ciation for Computational Linguistics (ACL),
%    pages 1870–1879. Vancouver, Canada.
%
%    \bibitem{FusionNet}Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, Weizhu Chen.
%    FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension. In Proceedings of the 
%    Sixth International Conference on Learning Representations (ICLR), 2018.
%
%    \bibitem{MatchLSTM}Shuohang Wang and Jing Jiang. Learning natural language inference with LSTM. In Proceedings of
%    the Conference on the North American Chapter of the Association for Computational Linguistics,
%    2016.
%    \bibitem{Pointer Networks}Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Proceedings of the Con-
%    ference on Advances in Neural Information Processing Systems, 2015.
%    \bibitem{RACE}Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale
%    reading comprehension dataset from examinations. In Proceedings of the 2017 Conference
%    on Empirical Methods in Natural Language Processing, pages 785–794, 2017.
%    \bibitem{SQuAD2}Rajpurkar, P., Jia, R., Liang, P.: Know what you don’t know: Unanswerable questions for
%    squad. arXiv preprint arXiv:1806.03822 (2018)
%    \bibitem{CBT}Hill, F., Bordes, A., Chopra, S., Weston, J.: The goldilocks principle: Reading children’s
%    books with explicit memory representations. arXiv preprint arXiv:1511.02301 (2015)
%    \bibitem{QANet}Yu, A.W., Dohan, D., Luong, M.T., Zhao, R., Chen, K., Norouzi, M., Le, Q.V.: Qanet:
%    Combining local convolution with global self-attention for reading comprehension. arXiv
%    preprint arXiv:1804.09541 (2018)
%
%    \bibitem{QuAC}Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang,
%    and Luke Zettlemoyer. Quac: Question answering in context. In Proceedings of the 2018
%    Conference on Empirical Methods in Natural Language Processing, pages 2174–2184, 2018.
%
%    \bibitem{DuReader}Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang,
%    Hua Wu, Qiaoqiao She, et al. Dureader: A chinese machine reading comprehension dataset
%    from real-world applications. In Proceedings of the Workshop on Machine Reading for Ques-
%    tion Answering, pages 37–46, 2018.
%
%    \bibitem{CoQA}Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering
%    challenge. arXiv preprint arXiv:1808.07042, 2018.
%    \bibitem{MS marco}Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng, L.:
%    Ms marco: A human generated machine reading comprehension dataset. arXiv preprint
%    arXiv:1611.09268 (2016)
%    \bibitem{NarrativeQA}Kočiskỳ, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.M., Melis, G., Grefenstette,
%    E.: The narrativeqa reading comprehension challenge. Transactions of the Association of
%    Computational Linguistics 6, 317–328 (2018)
%    \bibitem{Transformer}Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
%    Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing
%    Systems, 2017b.
%
%    \bibitem{ELMo}Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.:
%    Deep contextualized word representations. arXiv preprint arXiv:1802.05365 (2018)
%    \bibitem{GPT}Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understand-
%    ing with unsupervised learning. Tech. rep., Technical report, OpenAI (2018)
%    \bibitem{BERT}Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional
%    transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)
%
%    \bibitem{XLNet}Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V
%    Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint
%    arXiv:1906.08237, 2019.
%
%    \bibitem{Transformer-XL}Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,
%    and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length
%    context. arXiv preprint arXiv:1901.02860, 2019.
%
%    \bibitem{RoBERTa}Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
%    Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre-
%    training approach. arXiv preprint arXiv:1907.11692, 2019.
%
%    \bibitem{ALBERT}Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
%    Kevin Gimpel, Piyush Sharma, and Radu Soricut.
%    2020. ALBERT: A lite BERT for self-supervised
%    learning of language representations. In ICLR.
%    \bibitem{VQACo}Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention
%    for visual question answering. arXiv preprint arXiv:1606.00061, 2016.
%    \bibitem{UNILM}Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,
%    Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.
%    Unified language model pre-training for natural language un-
%    derstanding and generation. In NeurIPS, pages 13042–13054,
%    2019.
%    \bibitem{Co-matching}Shuohang Wang, Mo Yu, Jing Jiang, and Shiyu Chang.
%    2018. A Co-Matching Model for Multi-choice
%    Reading Comprehension. In Proceedings of the 56th
%    Annual Meeting of the Association for Computa-
%    tional Linguistics (Volume 2: Short Papers), pages
%    746–751. Association for Computational Linguis-
%    tics.
%
%    \bibitem{SNet}Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du, Weifeng Lv, and Ming Zhou. S-net: From
%    answer extraction to answer synthesis for machine reading comprehension. In Thirty-Second
%    AAAI Conference on Artificial Intelligence, 2018.
%    \bibitem{DCMN}Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, Xiang Zhou.
%    Dual Co-Matching Network for Multi-choice Reading Comprehension.
%    CoRR, abs/1901.09381
%
%    \bibitem{DCMN+}Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, Xiang Zhou.
%    DCMN+: Dual Co-Matching Network for Multi-choice Reading Comprehension.
%
%
%    \bibitem{SDNet}Chenguang Zhu, Michael Zeng, and Xuedong Huang.
%    2018. Sdnet: Contextualized attention-based deep
%    network for conversational question answering.
%    CoRR, abs/1812.03593.
%
%    \bibitem{simpleqa}Yasuhito Ohsugi, Itsumi Saito, Kyosuke Nishida, 
%    Hisako Asano, Junji Tomita. 
%    A Simple but Effective Method to Incorporate 
%    Multi-turn Context with BERT for Conversational 
%    Machine Comprehension. In
%    Annual Meeting of the Association for Compu-
%    tational Linguistics (ACL) 2019.
%
%    \bibitem{Retrospective}Zhuosheng Zhang, Junjie Yang, Hai Zhao. 2020.
%    Retrospective Reader for Machine Reading Comprehension. In AAAI
%
%
%
%
%
%
%
%
%\end{thebibliography}
%%\end{multicols}
