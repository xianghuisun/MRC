\documentclass{article}
%\documentclass{ctexart}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{lipsum}
%for long table
\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
 {}

\newenvironment{figurehere}
 {\def\@captype{figure}}
 {}
\makeatother
\usepackage{longtable}
%\documentclass{ctexart}
\usepackage{ctex}%中文
\usepackage{graphicx}
\usepackage{zhlipsum}
\usepackage{multirow}
\usepackage{biblatex}%biber才可以
\addbibresource{ass.bib}
\usepackage{cuted}
\graphicspath{{picture/}}
\usepackage[left=2.5cm,right=1.97cm,top=2.5cm,bottom=2.5cm]{geometry}%设置页边距
\renewcommand{\baselinestretch}{1.25}%行间距
%\usepackage[hidelinks,urlcolor=black,linkcolor=black]{hyperref}%引入超链接包，否则会出现Undefined sequence
\usepackage[hidelinks]{hyperref}
%[colorlinks,urlcolor=black,linkcolor=black]去除超链接中的颜色框
\usepackage{amssymb}%数学符号
\usepackage{amsmath}%数学公式
\usepackage{booktabs}%设置三线表的线粗细
\usepackage{array}%table
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
%\usepackage{cite}
% \ctexset{section={format={\zihao{3} \heiti \bfseries}},
% bibname={\zihao{-4} \heiti \bfseries 参考文献}}

\newcommand{\upcite}[1]{\textsuperscript{\textsuperscript{\cite{#1}}}}
\usepackage{caption}
\DeclareCaptionFont{heiti}{\heiti}
\captionsetup{labelsep=quad,font={small,bf,heiti},skip={4pt}}
\usepackage{geometry}
\title{\heiti \zihao{2} 神经机器阅读理解研究综述
\footnotetext{ 投稿日期: 2020-07-19 \\
\hspace*{1.8em}作者简介: 孙相会（1997-），男，硕士，研究方向为自然语言处理，E-mail: 2357094733@qq.com}
}%黑体2号 在标题那页插入脚注


\author{\zihao{-4} \songti 孙相会 \\ 东北大学 计算机科学与工程学院，沈阳 110169}
\date{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%












\begin{document}
    \maketitle %生成title,author,date

        \input{abstract_part.tex}
%----------------------正文-----------------------
%\begin{multicols}{2}
\section{引言}

%\vspace{10cm}在垂直方向上，两行之间的距离
机器阅读理解（Machine Reading Comprehension，MRC）作为一项衡量计算机阅读理解文本能力的任务，是自然语言处理领域（Natural Language Processing，NLP）
十分重要也是具有挑战性的研究方向。通常情况下，MRC任务就是
给定一篇或多篇文章，要求模型阅读这些文章后回答相关的问题。
早期的MRC系统主要是基于规则和模式匹配的方法，或者通过概率统计的方式计算问题与文章之间的相似程度。这很难达到深层次的理解文本，
而且数据集规模比较小，系统难以获得期望的性能也不能实际的应用。
随着深度学习的兴起以及NLP领域出现的一些经典方法
如Mikolov等人\upcite{word2vec}提出的word2vec，Bahdanau等人\upcite{Bahdanau}将注意力机制应用在NLP领域的机器翻译任务上等。这些技术的发展使得研究人员开始利用神经网络
构建机器阅读理解模型，
因此也叫神经机器阅读理解。Hermann等人\upcite{Hermann}在2015年发布了规模比以往数据集都要大的阅读理解数据集CNN\&Daily Mail，并且提出两个基于神经网络和注意力机制构建的模型（Attentive Reader，Impatinent Reader），这项工作可以视为机器阅读理解领域的奠基性工作。此后越来越多的学者在这两个模型的基础上构建更加复杂效果更好的神经机器阅读理解模型，而且规模越来越大，形式越来越复杂的数据集（如SQuAD\upcite{SQuAD1}，RACE\upcite{RACE}，MS MARCO\upcite{MSmarco}）也相继发布。
这使得MRC领域快速发展，模型的性能也逐渐提高。
自2018年随着ELMo\upcite{ELMo}、GPT\upcite{GPT}、BERT\upcite{BERT}等预训练模型的出现，
再一次提升了机器阅读理解模型的性能，甚至在某些数据集上模型的表现超过人类水平。

本文主要从MRC的具体任务概述出发，总共分5章，结构安排如下：
第2章介绍机器阅读理解的具体任务以及相应的评估指标；第3章
介绍经典神经机器阅读理解模型，对比它们的差异以及优缺点。同时介绍
近年来NLP领域最受关注的预训练模型以及如何应用在MRC任务上。
第4章针对更加复杂的机器阅读理解任务做概述。
第5章对MRC领域做总结与展望。

% \end{multicols}
% %在你想要分栏的段落上下加上begin end columns{2}


																																	

\input{dataset.tex}

\section{神经机器阅读理解模型}
随着大规模机器阅读理解数据集如CNN\&Daily Mail\upcite{CNNDailyMail}，SQuAD 1.1\upcite{SQuAD1}等的发布以及
深度学习技术的发展，神经机器阅读理解模型的性能显著的超过传统的基于规则和特征的模型，随着NLP领域预训练模型的发展，基于预训练模型
来做MRC任务的模型性能再一次的提升。
本章安排如下：3.1节简述基于深度学习的MRC模型通用框架，3.2节介绍基于注意力机制的模型，3.3节介绍目前流行的预训练模型，3.4节分析不同任务下答案预测层的设计。
\subsection{模型结构}
用于MRC任务的深度学习模型的整体框架主要包括如下几个层：词嵌入层、语义编码层、语义交互层、答案预测层。
\subsubsection{词嵌入层}
如何将文本有效的表示成计算机可以处理的形式同时可以有效地利用单词之间的语义一直的NLP领域
的重点问题。早期的one-hot形式编码用一个二值向量表示单词，但是存在数据稀疏并且随着单词个数的增加出现维度灾难的问题，此外这种形式的编码也不能够表示出单词之间的语义关系。

Rumelhart等人\upcite{Rumelhart}最早提出分布式表示的概念，
分布式表示是将单词用一个低维度的稠密向量表示，即将单词嵌入到一个低维向量空间中，因此这种表示方式也叫词嵌入。语义相近的单词在向量空间中距离也相近，因此这种词表示方法解决了one-hot编码的很多问题。
最流行的生成分布式词向量的技术如Word2Vec\upcite{word2vec}和GloVe\upcite{GloVe}。

但是这两种技术所训练出来的词向量是
静态的词向量，即训练好模型后一个单词的表示
向量就是固定的，没有考虑上下文的信息，因此无法解决多义词问题。为了解决这个问题，Peters等人\cite{ELMo}提出一种动态的基于上下文的词嵌入模型ELMo，每一个单词的词向量都是根据它所在的上下文语义表示的，很好的解决了一词多义的问题。关于ELMo以及预训练模型的细节见\ref{pretrain}节。

从早期的one-hot形式编码到分布式表示技术最后到基于上下文的词嵌入技术，每一种技术的出现都证明了一个好的文本表示方法可以
极大地提升模型的性能。
\subsubsection{语义编码层}
这一层的目的是在词嵌入层的基础上通过对词嵌入层的输入文本做特征提取，进一步获得句子层面的语义信息。常用的特征提取器
有基于RNN的变体如LSTM\upcite{LSTM}和GRU\upcite{GRU}等，因为这种循环结构适合处理文本这类序列数据以及非常适合作为语言模型的编码器。但也正是这种序列式的结构使得计算不能并行，训练耗时，更重要的是由于梯度消失问题不能解决单词之间长距离依赖问题，使得其
特征提取能力始终受限。Vaswani等人\upcite{Transformer}提出了一种用于机器翻译的encoder-decoder结构transformer，舍弃了常用的循环神经网络结构，完全的基于自注意力机制构建模型。实验表明transformer的特征提取能力强于循环神经网络而且可以并行计算。关于自注意力机制的细节见\ref{attention}节，关于transformer的细节见\ref{transformer}节。


%每一层由多头自注意力和前馈网络（FN）构成。Transformer的整体架构
%通过利用自注意力（self-attention）机制取代RNN那种序列式的计算方式，
%对于一个句子中的两个单词不考虑单词之间顺序的关系，直接计算它们之间的相关度，例如计算两个单词向量表示的内积。
%自注意力机制可以捕获句子中长距离依赖的特征关系，
%解决了循环神经网络固有的序列式传递信息导致后面的单词与前面的单词
%之间达不到有效的信息传递问题。
%通过自注意力机制不仅可以做到
%单词之间的全局交互同时其并行计算使得模型训练时间大幅减少。Transformer的encoder端和decoder端都可以做特征提取器如BERT\upcite{BERT}用encoder端特征提取，GPT\upcite{GPT}用decoder端特征提取，实验证明在大规模数据集上transformer的特征提取能力要强于
%基于RNNs\footnote{用RNNs来统一表示RNN的变体，如LSTM，GRU等\label{RNNs}}的编码器，目前几乎所有的NLP预训练模型都是利用transformer作为特征提取器。

\subsubsection{语义交互层}
在预测答案时需要将问题的语义信息与文章的语义信息关联，这样模型在预测答案时才能知道文章中哪一部分是问题的答案。
通常利用注意力机制实现这一目的，注意力机制就是让模型关注到重点的部分，不同的注意力计算方式很大程度上影响模型性能，
后面将详细介绍基于注意力机制的模型以及它们不同的计算方式。
\subsubsection{答案预测层}
这是整个模型架构的最后一层，用来输出预测的答案。如前面所提到的MRC任务大致分成四类，因此这一层的设计需要
考虑到答案形式。对于填空型任务，答案的输出是文章中的一个单词。对于多项选择任务，答案的输出是从多个候选答案中选择出正确的选项。
对于片段选择型任务，答案的输出是文章中某段连续的文本。对于自由答案型任务，答案的输出不限固定的文本，而是根据词典中的单词生成文本。
此外还有不可回答的问题，此时模型的输出还要考虑到问题是否可以回答。答案预测层的设计细节见\ref{output}节。

%下面介绍神经机器阅读理解模型中基于注意力机制的模型和基于预训练的模型。
%\input{embedding_layer.tex}



\input{interaction_layer.tex}
%
%
\input{pre_train.tex}
%% %\input{model.tex}
%
%
\input{output_layer.tex}

\begin{table}[ht]
	\centering
	\caption{模型对比（acc代表准确率）\\ Table 8 Model comparison(acc represents accuracy)}
	%\vspace{10pt}
	\begin{tabular}{l c c c}
		\toprule
		\multirow{2}{*}{模型}&SQuAD 1.1\upcite{SQuAD1}& SQuAD 2.0\upcite{SQuAD2} & RACE\upcite{RACE}\\
		\cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
		&EM/F1&EM/F1& acc \\
		\midrule
		Match-LSTM\upcite{MatchLSTM}& 64.7/73.7 & -&-\\
		\midrule
		DCN\upcite{DCN}& 66.2/75.9 &-&-\\
		\midrule
		ReasoNet\upcite{Reasonet}&70.6/79.4 &-&-\\
		\midrule
		BiDAF\upcite{BiDAF}&68.0/77.3 &-&-\\
		\midrule
		R-Net\upcite{RNet}&72.3/80.7 &-&-\\
		\midrule
		QANet\upcite{QANet}& 76.2/84.6&-&-\\
		\midrule
		ELMo+BiDAF\upcite{ELMo}&78.6/85.8&-&-\\
		\midrule
		GPT-v1\upcite{GPT}&-&-&59.0\\
		\midrule
		$\text{BERT}_{large}$\upcite{BERT}& 85.1/91.8 &80.0/83.1&72.0\\
		\midrule
		XLNet\upcite{XLNet}&89.9/95.1&86.4/89.1 &81.8\\
		\midrule
		RoBERTa\upcite{RoBERTa}&-&86.8/89.8 &83.2\\
		\midrule
		ALBERT\upcite{ALBERT}&-&88.1/90.9 &86.5\\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{小结}
本章首先介绍了神经机器阅读理解模型的结构，
涉及的技术如注意力机制以及相关的模型。
然后介绍了目前流行的几种预训练模型，最后对于不同的MRC任务分别介绍了各自输出层的
设计。

在预训练模型的基础上利用具体任务的数据微调模型，
即只需要稍微添加简单的输出层即可达到很好的效果。但是预训练模型只是给了更好的初始化参数，
如果想要进一步提升模型的性能还需要在其基础上根据具体的任务设计一个更好的模型。
%如Zhang等人\upcite{DCMN}
%利用BERT\upcite{BERT}和XLNet\upcite{XLNet}作为编码器同时采用文献\cite{Co-matching}提出的co-matching方法提出了DCMN模型，在RACE\upcite{RACE}
%数据集上达到了很高的准确率。而后在DCMN的基础上引入选项交互和段落选择任务使得
如何利用预训练模型结合具体任务改进模型的结构是至关重要的。表8对比了本章所介绍的模型在三个常用数据集上的表现，可以看到基于预训练
模型的效果要显著的优于其它模型，此外在其它数据集上排名靠前的模型几乎都是基于预训练模型的。
%\end{multicols}
%\begin{table}[ht]
%	\centering
%	\caption{模型对比（acc代表准确率）\\ Table 8 Model comparison(acc represents accuracy)}
%	%\vspace{10pt}
%	\begin{tabular}{l c c c}
%		\toprule
%		\multirow{2}{*}{模型}&SQuAD 1.1\upcite{SQuAD1}& SQuAD 2.0\upcite{SQuAD2} & RACE\upcite{RACE}\\
%		\cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
%		&EM/F1&EM/F1& acc \\
%		\midrule
%		Match-LSTM\upcite{MatchLSTM}& 64.7/73.7 & -&-\\
%		\midrule
%		DCN\upcite{DCN}& 66.2/75.9 &-&-\\
%		\midrule
%		ReasoNet\upcite{Reasonet}&70.6/79.4 &-&-\\
%		\midrule
%		BiDAF\upcite{BiDAF}&68.0/77.3 &-&-\\
%		\midrule
%		R-Net\upcite{RNet}&72.3/80.7 &-&-\\
%		\midrule
%		QANet\upcite{QANet}& 76.2/84.6&-&-\\
%		\midrule
%		ELMo+BiDAF\upcite{ELMo}&78.6/85.8&-&-\\
%		\midrule
%		GPT-v1\upcite{GPT}&-&-&59.0\\
%		\midrule
%		$\text{BERT}_{large}$\upcite{BERT}& 85.1/91.8 &80.0/83.1&72.0\\
%		\midrule
%		XLNet\upcite{XLNet}&89.9/95.1&86.4/89.1 &81.8\\
%		\midrule
%		RoBERTa\upcite{RoBERTa}&-&86.8/89.8 &83.2\\
%		\midrule
%		ALBERT\upcite{ALBERT}&-&88.1/90.9 &86.5\\
%		\bottomrule
%	\end{tabular}
%\end{table}

\input{new_trend.tex}
%
\section{总结与展望}
本文从机器阅读理解任务的定义出发，第二章介绍了不同任务下的数据集以及相应的评估标准。
第三章对经典的神经机器阅读理解模型进行了分析与研究，主要涉及模型
的整体框架以及所用到的方法如注意力机制、推理结构等。同时也总结了
目前一些主流的预训练模型，分析了它们之间的差异，列举了一些在预训练模型的基础上改进
的模型。通过各个模型的实验对比结果可以看到基于预训练的模型性能要显著的优于传统的仅仅基于
注意力机制的模型。第四章列举了一些目前MRC领域更加复杂的任务并且对每一种任务下相关的模型做了介绍。

机器阅读理解赋予了计算机阅读理解文本的能力，在搜索、对话、医疗以及教育领域都有着广阔的应用空间。
但是目前机器阅读理解仍然存在一些难题：

1）基于注意力机制的匹配模型大多是浅层的语义匹配模型，基于多跳结构的推理模式还过于单一，而
阅读理解是需要深层次的推理过程才能更好的理解文章，让模型具有较强的推理能力至关重要。

2）目前机器阅读理解领域的数据集大多是通用领域方向的，而设计专业领域数据集也尤为重要，更重要的是
这些适用于通用领域数据集的模型未必在专业领域有一样的性能。

3）生成答案的技术还需要进一步提升，回顾目前机器阅读理解领域的数据集以及相应的模型，
大多集中于片段选择式问答且模型准确度很高甚至超过人类水平。
而对于自由答案型这种需要生成答案的模型效果很差，有些模型直接将生成式问题转为抽取式问题，主要原因
在于生成答案模块的效果往往不好。

4）目前机器阅读理解主要集中于非结构化的文本领域，而
还有许多其它结构，不同模态的数据如表格、视频、音频、图片等，
多模态阅读理解模型也是未来的发展方向之一。




\printbibliography[title={参考文献}]




\end{document}