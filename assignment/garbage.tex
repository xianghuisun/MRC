%
%\begin{thebibliography}{99}
%    \bibitem{lstm}Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
%    1735–1780, 1997.
%
%    % \bibitem{BLEU}Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for au-
%    % tomatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on
%    % Association for Computational Linguistics, pages 311–318. Association for Computational
%    % Linguistics, 2002.
%    \bibitem{ROUGE}Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summariza-
%    tion Branches Out, 2004.
%
%    \bibitem{Maxout Network}Goodfellow, I.J., Warde-Farley, D., Mirza, M., Courville, A., Bengio, Y.: Maxout networks.
%    arXiv preprint arXiv:1302.4389 (2013)
%
%    \bibitem{word2vec}Mikolov T，Sutskever I，Chen K，et al.Distributed represen-
%    tations of words and phrases and their compositionality[C]//
%    Advances in Neural Information Processing Systems，
%    2013：3111-3119.
%
%    \bibitem{GRU}Junyoung Chung, Caglar Gulcehre,
%    Kyunghyun Cho, and Yoshua Bengio. 2014. Empir-
%    ical Evaluation of Gated Recurrent Neural Networks
%    on Sequence Modeling. arXiv, pages 1–9.
%
%    \bibitem{GloVe}Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for
%    word//w representation. In Empirical Methods in Natural Language Processing (EMNLP), pp.
%    1532–1543, 2014.
%
%    \bibitem{neural machine translation by jointly learning to align and translate}Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
%    learning to align and translate. ICLR, 2015.
%
%    \bibitem{memory network}J. Weston, S. Chopra, and A. Bordes. Memory networks. In International Conference on
%    Learning Representations (ICLR), 2015.
%
%    \bibitem{Highway Network}Srivastava, R.K., Greff, K., Schmidhuber, J.: Highway networks.
%    arXiv preprint
%    arXiv:1505.00387 (2015)
%
%    \bibitem{MemN2N}Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In
%    Advances in neural information processing systems, pages 2440–2448, 2015.
%
%    \bibitem{Teaching Machines to Read and Comprehend}Hermann, K.M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., Blun-
%    som, P.: Teaching machines to read and comprehend. In: Advances in Neural Information
%    Processing Systems, pp. 1693–1701 (2015)
%
%    \bibitem{AR}Danqi Chen, Jason Bolton, and Christopher D Manning. A thorough examination of the
%    cnn/daily mail reading comprehension task. In Proceedings of the 54th Annual Meeting of
%    the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages
%    2358–2367, 2016.
%
%    \bibitem{ASR}Rudolf Kadlec, Martin Schmid, Ondřej Bajgar, and Jan Kleindienst. Text understanding
%    with the attention sum reader network. In Proceedings of the 54th Annual Meeting of the
%    Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 908–
%    918, 2016.
%
%    \bibitem{IAReader}Alessandro Sordoni, Philip Bachman, Adam Trischler, and Yoshua Bengio. Iterative alternat-
%    ing neural attention for machine reading. arXiv preprint arXiv:1606.02245, 2016.
%
%    \bibitem{SQuAD1}Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
%    for machine comprehension of text. In Proceedings of the Conference on Empirical Methods in
%    Natural Language Processing, 2016.
%
%    \bibitem{sentinel vector}Merity, S., Xiong, C., Bradbury, J., Socher, R.: Pointer sentinel mixture models. arXiv
%    preprint arXiv:1609.07843 (2016)
%
%    \bibitem{Deepwise Separable convolution}François Chollet. Xception: Deep learning with depthwise separable convolutions.
%    abs/1610.02357, 2016.
%
%    \bibitem{Machine comprehension using match-lstm and answer pointer}Wang, S., Jiang, J.: Machine comprehension using match-lstm and answer pointer. arXiv
%    preprint arXiv:1608.07905 (2016)
%
%    \bibitem{Dynamic coattention networks for question answering}Xiong, C., Zhong, V., Socher, R.: Dynamic coattention networks for question answering.
%    arXiv preprint arXiv:1611.01604 (2016)
%
%    \bibitem{Bidirectional attention flow for machine comprehension}Seo, M., Kembhavi, A., Farhadi, A., Hajishirzi, H.: Bidirectional attention flow for machine
%    comprehension. arXiv preprint arXiv:1611.01603 (2016)
%    
%    \bibitem{RNet}Wang, W., Yang, N., Wei, F., Chang, B., Zhou, M.: Gated self-matching networks for
%    reading comprehension and question answering. In: Proceedings of the 55th Annual Meet-
%    ing of the Association for Computational Linguistics (Volume 1: Long Papers), vol. 1, pp.
%    189–198 (2017)
%
%    \bibitem{Reasonet}Shen, Y., Huang, P.S., Gao, J., Chen, W.: Reasonet: Learning to stop reading in machine
%    comprehension. In: Proceedings of the 23rd ACM SIGKDD International Conference on
%    Knowledge Discovery and Data Mining, pp. 1047–1055. ACM (2017)
%
%    \bibitem{GAReader}Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William Cohen, and Ruslan Salakhutdinov.
%    Gated-attention readers for text comprehension. In Proceedings of the 55th Annual Meeting
%    of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1832–1846,
%    2017.
%
%    \bibitem{PGNet}Abigail See, Peter J. Liu, and Christopher D.
%    Manning. 2017. Get to the point: Summa-
%    rization with pointer-generator networks. In
%    Annual Meeting of the Association for Compu-
%    tational Linguistics (ACL), pages 1073–1083.
%    Vancouver, Canada.
%
%    \bibitem{TriviaQA}Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: A large scale
%    distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th
%    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
%    volume 1, pages 1601–1611, 2017.
%
%    \bibitem{DrQA}Danqi Chen, Adam Fisch, Jason Weston, and
%    Antoine Bordes. 2017. Reading Wikipedia
%    to answer open-domain questions. In Asso-
%    ciation for Computational Linguistics (ACL),
%    pages 1870–1879. Vancouver, Canada.
%
%    \bibitem{FusionNet}Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, Weizhu Chen.
%    FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension. In Proceedings of the 
%    Sixth International Conference on Learning Representations (ICLR), 2018.
%
%    \bibitem{MatchLSTM}Shuohang Wang and Jing Jiang. Learning natural language inference with LSTM. In Proceedings of
%    the Conference on the North American Chapter of the Association for Computational Linguistics,
%    2016.
%    \bibitem{Pointer Networks}Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Proceedings of the Con-
%    ference on Advances in Neural Information Processing Systems, 2015.
%    \bibitem{RACE}Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale
%    reading comprehension dataset from examinations. In Proceedings of the 2017 Conference
%    on Empirical Methods in Natural Language Processing, pages 785–794, 2017.
%    \bibitem{SQuAD2}Rajpurkar, P., Jia, R., Liang, P.: Know what you don’t know: Unanswerable questions for
%    squad. arXiv preprint arXiv:1806.03822 (2018)
%    \bibitem{CBT}Hill, F., Bordes, A., Chopra, S., Weston, J.: The goldilocks principle: Reading children’s
%    books with explicit memory representations. arXiv preprint arXiv:1511.02301 (2015)
%    \bibitem{QANet}Yu, A.W., Dohan, D., Luong, M.T., Zhao, R., Chen, K., Norouzi, M., Le, Q.V.: Qanet:
%    Combining local convolution with global self-attention for reading comprehension. arXiv
%    preprint arXiv:1804.09541 (2018)
%
%    \bibitem{QuAC}Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang,
%    and Luke Zettlemoyer. Quac: Question answering in context. In Proceedings of the 2018
%    Conference on Empirical Methods in Natural Language Processing, pages 2174–2184, 2018.
%
%    \bibitem{DuReader}Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang,
%    Hua Wu, Qiaoqiao She, et al. Dureader: A chinese machine reading comprehension dataset
%    from real-world applications. In Proceedings of the Workshop on Machine Reading for Ques-
%    tion Answering, pages 37–46, 2018.
%
%    \bibitem{CoQA}Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering
%    challenge. arXiv preprint arXiv:1808.07042, 2018.
%    \bibitem{MS marco}Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng, L.:
%    Ms marco: A human generated machine reading comprehension dataset. arXiv preprint
%    arXiv:1611.09268 (2016)
%    \bibitem{NarrativeQA}Kočiskỳ, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.M., Melis, G., Grefenstette,
%    E.: The narrativeqa reading comprehension challenge. Transactions of the Association of
%    Computational Linguistics 6, 317–328 (2018)
%    \bibitem{Transformer}Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
%    Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing
%    Systems, 2017b.
%
%    \bibitem{ELMo}Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.:
%    Deep contextualized word representations. arXiv preprint arXiv:1802.05365 (2018)
%    \bibitem{GPT}Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understand-
%    ing with unsupervised learning. Tech. rep., Technical report, OpenAI (2018)
%    \bibitem{BERT}Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional
%    transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)
%
%    \bibitem{XLNet}Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V
%    Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint
%    arXiv:1906.08237, 2019.
%
%    \bibitem{Transformer-XL}Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,
%    and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length
%    context. arXiv preprint arXiv:1901.02860, 2019.
%
%    \bibitem{RoBERTa}Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
%    Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre-
%    training approach. arXiv preprint arXiv:1907.11692, 2019.
%
%    \bibitem{ALBERT}Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
%    Kevin Gimpel, Piyush Sharma, and Radu Soricut.
%    2020. ALBERT: A lite BERT for self-supervised
%    learning of language representations. In ICLR.
%    \bibitem{VQACo}Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention
%    for visual question answering. arXiv preprint arXiv:1606.00061, 2016.
%    \bibitem{UNILM}Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,
%    Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.
%    Unified language model pre-training for natural language un-
%    derstanding and generation. In NeurIPS, pages 13042–13054,
%    2019.
%    \bibitem{Co-matching}Shuohang Wang, Mo Yu, Jing Jiang, and Shiyu Chang.
%    2018. A Co-Matching Model for Multi-choice
%    Reading Comprehension. In Proceedings of the 56th
%    Annual Meeting of the Association for Computa-
%    tional Linguistics (Volume 2: Short Papers), pages
%    746–751. Association for Computational Linguis-
%    tics.
%
%    \bibitem{SNet}Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du, Weifeng Lv, and Ming Zhou. S-net: From
%    answer extraction to answer synthesis for machine reading comprehension. In Thirty-Second
%    AAAI Conference on Artificial Intelligence, 2018.
%    \bibitem{DCMN}Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, Xiang Zhou.
%    Dual Co-Matching Network for Multi-choice Reading Comprehension.
%    CoRR, abs/1901.09381
%
%    \bibitem{DCMN+}Shuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng Zhang, Xi Zhou, Xiang Zhou.
%    DCMN+: Dual Co-Matching Network for Multi-choice Reading Comprehension.
%
%
%    \bibitem{SDNet}Chenguang Zhu, Michael Zeng, and Xuedong Huang.
%    2018. Sdnet: Contextualized attention-based deep
%    network for conversational question answering.
%    CoRR, abs/1812.03593.
%
%    \bibitem{simpleqa}Yasuhito Ohsugi, Itsumi Saito, Kyosuke Nishida, 
%    Hisako Asano, Junji Tomita. 
%    A Simple but Effective Method to Incorporate 
%    Multi-turn Context with BERT for Conversational 
%    Machine Comprehension. In
%    Annual Meeting of the Association for Compu-
%    tational Linguistics (ACL) 2019.
%
%    \bibitem{Retrospective}Zhuosheng Zhang, Junjie Yang, Hai Zhao. 2020.
%    Retrospective Reader for Machine Reading Comprehension. In AAAI
%
%
%
%
%
%
%
%
%\end{thebibliography}
%%\end{multicols}
